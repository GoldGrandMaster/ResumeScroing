{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule based Scoring using the NER model trained using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "from tika import parser\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E002] Can't find factory for 'transformer' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, ner, beam_ner, entity_ruler, tagger, morphologizer, senter, sentencizer, textcat, spancat, spancat_singlelabel, future_entity_ruler, span_ruler, textcat_multilabel, en.lemmatizer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ner_model \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mdirname(os\u001b[39m.\u001b[39;49mgetcwd()),\u001b[39m\"\u001b[39;49m\u001b[39mD:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mMy_projects\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mResumeScroing\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mResume_scorer\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mTraining_NER\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mmodel-last\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\Passion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m     55\u001b[0m         name,\n\u001b[0;32m     56\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m     57\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m     58\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m     59\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m     60\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m     61\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Passion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:444\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m load_model_from_package(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[39mif\u001b[39;00m Path(name)\u001b[39m.\u001b[39mexists():  \u001b[39m# path to model data directory\u001b[39;00m\n\u001b[1;32m--> 444\u001b[0m         \u001b[39mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(name, \u001b[39m\"\u001b[39m\u001b[39mexists\u001b[39m\u001b[39m\"\u001b[39m):  \u001b[39m# Path or Path-like to model data\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_path(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Passion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:516\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    514\u001b[0m overrides \u001b[39m=\u001b[39m dict_to_dot(config)\n\u001b[0;32m    515\u001b[0m config \u001b[39m=\u001b[39m load_config(config_path, overrides\u001b[39m=\u001b[39moverrides)\n\u001b[1;32m--> 516\u001b[0m nlp \u001b[39m=\u001b[39m load_model_from_config(\n\u001b[0;32m    517\u001b[0m     config,\n\u001b[0;32m    518\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m    519\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m    520\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m    521\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m    522\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[0;32m    523\u001b[0m )\n\u001b[0;32m    524\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\u001b[39m.\u001b[39mfrom_disk(model_path, exclude\u001b[39m=\u001b[39mexclude, overrides\u001b[39m=\u001b[39moverrides)\n",
      "File \u001b[1;32mc:\\Users\\Passion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:564\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[1;34m(config, meta, vocab, disable, enable, exclude, auto_fill, validate)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[39m# This will automatically handle all codes registered via the languages\u001b[39;00m\n\u001b[0;32m    562\u001b[0m \u001b[39m# registry, including custom subclasses provided via entry points\u001b[39;00m\n\u001b[0;32m    563\u001b[0m lang_cls \u001b[39m=\u001b[39m get_lang_class(nlp_config[\u001b[39m\"\u001b[39m\u001b[39mlang\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> 564\u001b[0m nlp \u001b[39m=\u001b[39m lang_cls\u001b[39m.\u001b[39;49mfrom_config(\n\u001b[0;32m    565\u001b[0m     config,\n\u001b[0;32m    566\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m    567\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m    568\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m    569\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m    570\u001b[0m     auto_fill\u001b[39m=\u001b[39;49mauto_fill,\n\u001b[0;32m    571\u001b[0m     validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    572\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[0;32m    573\u001b[0m )\n\u001b[0;32m    574\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\n",
      "File \u001b[1;32mc:\\Users\\Passion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\language.py:1803\u001b[0m, in \u001b[0;36mLanguage.from_config\u001b[1;34m(cls, config, vocab, disable, enable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[0;32m   1800\u001b[0m     factory \u001b[39m=\u001b[39m pipe_cfg\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mfactory\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1801\u001b[0m     \u001b[39m# The pipe name (key in the config) here is the unique name\u001b[39;00m\n\u001b[0;32m   1802\u001b[0m     \u001b[39m# of the component, not necessarily the factory\u001b[39;00m\n\u001b[1;32m-> 1803\u001b[0m     nlp\u001b[39m.\u001b[39;49madd_pipe(\n\u001b[0;32m   1804\u001b[0m         factory,\n\u001b[0;32m   1805\u001b[0m         name\u001b[39m=\u001b[39;49mpipe_name,\n\u001b[0;32m   1806\u001b[0m         config\u001b[39m=\u001b[39;49mpipe_cfg,\n\u001b[0;32m   1807\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m   1808\u001b[0m         raw_config\u001b[39m=\u001b[39;49mraw_config,\n\u001b[0;32m   1809\u001b[0m     )\n\u001b[0;32m   1810\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1811\u001b[0m     \u001b[39m# We need the sourced components to reference the same\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[39m# vocab without modifying the current vocab state **AND**\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1817\u001b[0m     \u001b[39m# during deserialization, so they do not need any\u001b[39;00m\n\u001b[0;32m   1818\u001b[0m     \u001b[39m# additional handling.\u001b[39;00m\n\u001b[0;32m   1819\u001b[0m     \u001b[39mif\u001b[39;00m vocab_b \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Passion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\language.py:786\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    782\u001b[0m     pipe_component, factory_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_pipe_from_source(\n\u001b[0;32m    783\u001b[0m         factory_name, source, name\u001b[39m=\u001b[39mname\n\u001b[0;32m    784\u001b[0m     )\n\u001b[0;32m    785\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 786\u001b[0m     pipe_component \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_pipe(\n\u001b[0;32m    787\u001b[0m         factory_name,\n\u001b[0;32m    788\u001b[0m         name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    789\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    790\u001b[0m         raw_config\u001b[39m=\u001b[39;49mraw_config,\n\u001b[0;32m    791\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    792\u001b[0m     )\n\u001b[0;32m    793\u001b[0m pipe_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_pipe_index(before, after, first, last)\n\u001b[0;32m    794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipe_meta[name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_factory_meta(factory_name)\n",
      "File \u001b[1;32mc:\\Users\\Passion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\language.py:660\u001b[0m, in \u001b[0;36mLanguage.create_pipe\u001b[1;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_factory(factory_name):\n\u001b[0;32m    653\u001b[0m     err \u001b[39m=\u001b[39m Errors\u001b[39m.\u001b[39mE002\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    654\u001b[0m         name\u001b[39m=\u001b[39mfactory_name,\n\u001b[0;32m    655\u001b[0m         opts\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfactory_names),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    658\u001b[0m         lang_code\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlang,\n\u001b[0;32m    659\u001b[0m     )\n\u001b[1;32m--> 660\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err)\n\u001b[0;32m    661\u001b[0m pipe_meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_factory_meta(factory_name)\n\u001b[0;32m    662\u001b[0m \u001b[39m# This is unideal, but the alternative would mean you always need to\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[39m# specify the full config settings, which is not really viable.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: [E002] Can't find factory for 'transformer' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, ner, beam_ner, entity_ruler, tagger, morphologizer, senter, sentencizer, textcat, spancat, spancat_singlelabel, future_entity_ruler, span_ruler, textcat_multilabel, en.lemmatizer"
     ]
    }
   ],
   "source": [
    "ner_model = spacy.load(os.path.join(os.path.dirname(os.getcwd()),\"D:\\My_projects\\ResumeScroing\\Resume_scorer\\Training_NER\\model-last\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a test resume and the JD of Borneo for testing out the scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 22:31:03,423 [MainThread  ] [ERROR]  Unable to run java; is it installed?\n",
      "2023-03-27 22:31:03,424 [MainThread  ] [ERROR]  Failed to receive startup confirmation from startServer.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to start Tika server.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m parser_jd \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39;49mfrom_file(\u001b[39m\"\u001b[39;49m\u001b[39mborneo-JD.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m parser_resume \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mfrom_file(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(os\u001b[39m.\u001b[39mgetcwd()),\\\n\u001b[0;32m      3\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mTraining_NER/dataset/test/Resume.docx\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      5\u001b[0m jd \u001b[39m=\u001b[39m parser_jd[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\YGP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tika\\parser.py:40\u001b[0m, in \u001b[0;36mfrom_file\u001b[1;34m(filename, serverEndpoint, service, xmlContent, headers, config_path, requestOptions, raw_response)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mParses a file for metadata and content\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m:param filename: path to file which needs to be parsed or binary file using open(path,'rb')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39m        'content' has a str value and metadata has a dict type value.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m xmlContent:\n\u001b[1;32m---> 40\u001b[0m     output \u001b[39m=\u001b[39m parse1(service, filename, serverEndpoint, headers\u001b[39m=\u001b[39;49mheaders, config_path\u001b[39m=\u001b[39;49mconfig_path, requestOptions\u001b[39m=\u001b[39;49mrequestOptions)\n\u001b[0;32m     41\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     output \u001b[39m=\u001b[39m parse1(service, filename, serverEndpoint, services\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/meta\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/tika\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/rmeta/xml\u001b[39m\u001b[39m'\u001b[39m},\n\u001b[0;32m     43\u001b[0m                         headers\u001b[39m=\u001b[39mheaders, config_path\u001b[39m=\u001b[39mconfig_path, requestOptions\u001b[39m=\u001b[39mrequestOptions)\n",
      "File \u001b[1;32mc:\\Users\\YGP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tika\\tika.py:337\u001b[0m, in \u001b[0;36mparse1\u001b[1;34m(option, urlOrPath, serverEndpoint, verbose, tikaServerJar, responseMimeType, services, rawResponse, headers, config_path, requestOptions)\u001b[0m\n\u001b[0;32m    335\u001b[0m headers\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mAccept\u001b[39m\u001b[39m'\u001b[39m: responseMimeType, \u001b[39m'\u001b[39m\u001b[39mContent-Disposition\u001b[39m\u001b[39m'\u001b[39m: make_content_disposition_header(path\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39mis\u001b[39;00m unicode_string \u001b[39melse\u001b[39;00m path)})\n\u001b[0;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m urlOrPath \u001b[39mif\u001b[39;00m _is_file_object(urlOrPath) \u001b[39melse\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 337\u001b[0m     status, response \u001b[39m=\u001b[39m callServer(\u001b[39m'\u001b[39;49m\u001b[39mput\u001b[39;49m\u001b[39m'\u001b[39;49m, serverEndpoint, service, f,\n\u001b[0;32m    338\u001b[0m                                   headers, verbose, tikaServerJar, config_path\u001b[39m=\u001b[39;49mconfig_path,\n\u001b[0;32m    339\u001b[0m                                   rawResponse\u001b[39m=\u001b[39;49mrawResponse, requestOptions\u001b[39m=\u001b[39;49mrequestOptions)\n\u001b[0;32m    341\u001b[0m \u001b[39mif\u001b[39;00m file_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mremote\u001b[39m\u001b[39m'\u001b[39m: os\u001b[39m.\u001b[39munlink(path)\n\u001b[0;32m    342\u001b[0m \u001b[39mreturn\u001b[39;00m (status, response)\n",
      "File \u001b[1;32mc:\\Users\\YGP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tika\\tika.py:532\u001b[0m, in \u001b[0;36mcallServer\u001b[1;34m(verb, serverEndpoint, service, data, headers, verbose, tikaServerJar, httpVerbs, classpath, rawResponse, config_path, requestOptions)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[39mglobal\u001b[39;00m TikaClientOnly\n\u001b[0;32m    531\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m TikaClientOnly:\n\u001b[1;32m--> 532\u001b[0m     serverEndpoint \u001b[39m=\u001b[39m checkTikaServer(scheme, serverHost, port, tikaServerJar, classpath, config_path)\n\u001b[0;32m    534\u001b[0m serviceUrl  \u001b[39m=\u001b[39m serverEndpoint \u001b[39m+\u001b[39m service\n\u001b[0;32m    535\u001b[0m \u001b[39mif\u001b[39;00m verb \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m httpVerbs:\n",
      "File \u001b[1;32mc:\\Users\\YGP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tika\\tika.py:602\u001b[0m, in \u001b[0;36mcheckTikaServer\u001b[1;34m(scheme, serverHost, port, tikaServerJar, classpath, config_path)\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m status:\n\u001b[0;32m    601\u001b[0m             log\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mFailed to receive startup confirmation from startServer.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 602\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to start Tika server.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    603\u001b[0m \u001b[39mreturn\u001b[39;00m serverEndpoint\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to start Tika server."
     ]
    }
   ],
   "source": [
    "parser_jd = parser.from_file(\"borneo-JD.txt\")\n",
    "parser_resume = parser.from_file(os.path.join(os.path.dirname(os.getcwd()),\\\n",
    "\"Training_NER/dataset/test/Resume.docx\"))\n",
    "\n",
    "jd = parser_jd[\"content\"]\n",
    "resume = parser_resume[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Scoring routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    doc = doc.replace(\"\\n\", \" \")\n",
    "    doc = doc.replace(\"•\",\"\")\n",
    "#     doc = doc.replace(\"\")\n",
    "    doc = doc.replace(\"–\",\"\")\n",
    "    doc = doc.replace(\"\\t\",\" \")\n",
    "    doc = doc.strip()\n",
    "    return doc\n",
    "\n",
    "jd = preprocess(jd)\n",
    "resume = preprocess(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_score = 0\n",
    "\n",
    "for word in resume.split(\" \"):\n",
    "    if word.strip() in [\"PhD\",\"PHD\",\"Research Associate\"]:\n",
    "        deg_score=3\n",
    "    elif word.strip() in [\"MS\",\"MT\",\"M.Tech\",\"Masters\"]:\n",
    "        if deg_score<2:\n",
    "            deg_score=2\n",
    "    elif word.strip() in [\"BS\",\"BE\",\"B.S\",\"B.E\",\"B.Tech\",\"Bachelors\"]:\n",
    "        if deg_score<1:\n",
    "            deg_score=1\n",
    "\n",
    "# print(deg_score)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_score = 0\n",
    "\n",
    "for word in resume.split(\" \"):\n",
    "    if word.strip() in [\"Sr.\",\"Senior\"]:\n",
    "        if des_score<3:\n",
    "            des_score=3\n",
    "    elif word.strip() in [\"Associate\", \"Scientist\", \"Engineer\"]:\n",
    "        if des_score<2:\n",
    "            des_score=2\n",
    "    elif word.strip() in [\"Analyst\", \"Junior\"]:\n",
    "        if des_score<1:\n",
    "            des_score=1\n",
    "\n",
    "# print(des_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_score = 0\n",
    "a = re.findall(r'[0-9]+\\+*[ ]?[Yy]ear',resume)\n",
    "a.sort()\n",
    "\n",
    "if len(a)>0:\n",
    "    exp = a[len(a)-1].lower().split(\"y\")[0].strip()\n",
    "#     print(exp)\n",
    "\n",
    "    if \"+\" in exp :\n",
    "        exp = exp[:-1]\n",
    "    \n",
    "    exp = int(exp)\n",
    "#     print(exp)\n",
    "    if exp>=4:\n",
    "        exp_score=3\n",
    "    elif exp>=2:\n",
    "        exp_score=2\n",
    "    elif exp==1:\n",
    "        exp_score=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Document Similarity for comparing resume and skills with the JD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abhinaykumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import string\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# splitting the text lines into words\n",
    "# translation table is a global variable\n",
    "# mapping upper case to lower case and\n",
    "# punctuation to spaces\n",
    "translation_table = str.maketrans(string.punctuation+string.ascii_uppercase,\n",
    "\t\t\t\t\t\t\t\t\t\" \"*len(string.punctuation)+string.ascii_lowercase)\n",
    "\t\n",
    "# returns a list of the words\n",
    "# in the file\n",
    "def get_words_from_line_list(text):\n",
    "\t\n",
    "\ttext = text.translate(translation_table)\n",
    "\tword_list = [x for x in text.split() if x not in set(stopwords.words('english'))]\n",
    "\t\n",
    "\treturn word_list\n",
    "\n",
    "\n",
    "# counts frequency of each word\n",
    "# returns a dictionary which maps\n",
    "# the words to their frequency.\n",
    "def count_frequency(word_list):\n",
    "\t\n",
    "\tD = {}\n",
    "\t\n",
    "\tfor new_word in word_list:\n",
    "\t\t\n",
    "\t\tif new_word in D:\n",
    "\t\t\tD[new_word] = D[new_word] + 1\n",
    "\t\t\t\n",
    "\t\telse:\n",
    "\t\t\tD[new_word] = 1\n",
    "\t\t\t\n",
    "\treturn D\n",
    "\n",
    "# returns dictionary of (word, frequency)\n",
    "# pairs from the previous dictionary.\n",
    "def word_frequencies_for_text(text):\n",
    "\t\n",
    "\tline_list = text\n",
    "\tword_list = get_words_from_line_list(line_list)\n",
    "\tfreq_mapping = count_frequency(word_list)\n",
    "\n",
    "# \tprint(\"File\", filename, \":\", )\n",
    "# \tprint(len(line_list), \"lines, \", )\n",
    "# \tprint(len(word_list), \"words, \", )\n",
    "# \tprint(len(freq_mapping), \"distinct words\")\n",
    "\n",
    "\treturn freq_mapping\n",
    "\n",
    "\n",
    "# returns the dot product of two documents\n",
    "def dotProduct(D1, D2):\n",
    "\tSum = 0.0\n",
    "\t\n",
    "\tfor key in D1:\n",
    "\t\t\n",
    "\t\tif key in D2:\n",
    "\t\t\tSum += (D1[key] * D2[key])\n",
    "\t\t\t\n",
    "\treturn Sum\n",
    "\n",
    "# returns the angle in radians\n",
    "# between document vectors\n",
    "def vector_angle(D1, D2):\n",
    "\tnumerator = dotProduct(D1, D2)\n",
    "\tdenominator = math.sqrt(dotProduct(D1, D1)*dotProduct(D2, D2))\n",
    "\t\n",
    "\treturn math.acos(numerator / denominator)\n",
    "\n",
    "\n",
    "def documentSimilarity(text_1, text_2):\n",
    "    sorted_word_list_1 = word_frequencies_for_text(text_1)\n",
    "    sorted_word_list_2 = word_frequencies_for_text(text_2)\n",
    "    distance = vector_angle(sorted_word_list_1, sorted_word_list_2)\n",
    "    return math.degrees(distance)\n",
    "\t\n",
    "# Driver code\n",
    "# documentSimilarity(jd, resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = ner_model(resume)\n",
    "\n",
    "# skill_list = [tok.text for tok in doc if tok.ent_type_==\"Skills\"]\n",
    "# skill_text = \" \".join(skill_list)\n",
    "\n",
    "# skill_score = 0\n",
    "\n",
    "# if len(skill_list)>0:\n",
    "#     skill_match = 90.0-documentSimilarity(jd,skill_text)\n",
    "#     ## Skills are matched on a scale of 0-10\n",
    "#     skill_score = min(10,skill_match)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Resume Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_match = 90-documentSimilarity(jd, resume)\n",
    "resume_score = min(20,resume_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Score (on scale 1 to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8\n"
     ]
    }
   ],
   "source": [
    "score = round(10/7*(deg_score*0.20+des_score*0.20+exp_score*0.20+skill_score*0.30+resume_score*0.10),1)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b1b70dabb37e9fb1db1c4f6ebfc24535df5e6d7850a91906a7d79f2982b65cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
